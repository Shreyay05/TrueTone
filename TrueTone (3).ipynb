{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siNKmMsp0l75",
        "outputId": "e08d6bef-8a33-4298-b0e8-8bfcc57b426f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utwKr0um69Dh",
        "outputId": "933b3799-3708-4b91-bc0d-9dd305948235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Unzipped to /content/img_align_celeba/\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/image_align_celeba.zip\"  # ⬅️ change path here\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/img_align_celeba\")\n",
        "\n",
        "print(\"✅ Unzipped to /content/img_align_celeba/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "duXaWA-V7jPz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e7aPr3qL7qIX"
      },
      "outputs": [],
      "source": [
        "# Load CSVs\n",
        "skin_df = pd.read_csv(\"/content/drive/MyDrive/celeba_labeled_skin_full.csv\")\n",
        "shade_df = pd.read_csv(\"/content/drive/MyDrive/foundation_shade_mapping_expanded.csv\")\n",
        "\n",
        "# Merge to get recommended shades\n",
        "merged_df = pd.merge(skin_df, shade_df, on=[\"SkinTone\", \"Undertone\"], how=\"left\")\n",
        "merged_df.dropna(subset=[\"RecommendedShades\"], inplace=True)\n",
        "\n",
        "# Label Encoding\n",
        "skin_enc = LabelEncoder()\n",
        "under_enc = LabelEncoder()\n",
        "shade_enc = LabelEncoder()\n",
        "\n",
        "merged_df[\"SkinToneClass\"] = skin_enc.fit_transform(merged_df[\"SkinTone\"])\n",
        "merged_df[\"UndertoneClass\"] = under_enc.fit_transform(merged_df[\"Undertone\"])\n",
        "merged_df[\"ShadeClass\"] = shade_enc.fit_transform(merged_df[\"RecommendedShades\"])\n",
        "\n",
        "# Split\n",
        "train_df, val_df = train_test_split(merged_df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Image directory\n",
        "image_dir = \"/content/img_align_celeba/img_align_celeba/img_align_celeba\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qW5PJr9673km"
      },
      "outputs": [],
      "source": [
        "class CelebaShadeDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transform=None):\n",
        "        self.df = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.image_dir, row[\"Image\"])\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        skin_label = row[\"SkinToneClass\"]\n",
        "        under_label = row[\"UndertoneClass\"]\n",
        "        shade_label = row[\"ShadeClass\"]\n",
        "        return image, skin_label, under_label, shade_label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T5WIKQVe771j"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "train_dataset = CelebaShadeDataset(train_df, image_dir, transform)\n",
        "val_dataset = CelebaShadeDataset(val_df, image_dir, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "paXY4hEf8EUx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class ShadePredictorCNN(nn.Module):\n",
        "    def __init__(self, num_skin, num_under, num_shade):\n",
        "        super(ShadePredictorCNN, self).__init__()\n",
        "\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        self.base = nn.Sequential(*list(resnet.children())[:-1])  # remove the FC layer\n",
        "\n",
        "        self.head_skin = nn.Linear(512, num_skin)\n",
        "        self.head_under = nn.Linear(512, num_under)\n",
        "        self.head_shade = nn.Linear(512, num_shade)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base(x)              # output shape: (batch_size, 512, 1, 1)\n",
        "        x = torch.flatten(x, 1)       # shape: (batch_size, 512)\n",
        "\n",
        "        out_skin = self.head_skin(x)\n",
        "        out_under = self.head_under(x)\n",
        "        out_shade = self.head_shade(x)\n",
        "\n",
        "        return out_skin, out_under, out_shade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRpcMcQm8F1o",
        "outputId": "cdc096f2-194b-4fa6-b5f2-8fef8e354bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 130MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = ShadePredictorCNN(\n",
        "    num_skin=len(skin_enc.classes_),\n",
        "    num_under=len(under_enc.classes_),\n",
        "    num_shade=len(shade_enc.classes_)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B4TPHwDp8KbV"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion_skin = nn.CrossEntropyLoss()\n",
        "criterion_under = nn.CrossEntropyLoss()\n",
        "criterion_shade = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI5H0Jc88SrW",
        "outputId": "c497d976-6ca4-4a3b-eff8-439aafead0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2839/2839 [10:29<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Loss: 3784.2523 | Skin Acc: 0.8141 | Undertone Acc: 0.9049 | Shade Acc: 0.7406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2839/2839 [10:22<00:00,  4.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 | Loss: 2889.7344 | Skin Acc: 0.8543 | Undertone Acc: 0.9302 | Shade Acc: 0.7962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2839/2839 [10:28<00:00,  4.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 | Loss: 2486.5684 | Skin Acc: 0.8761 | Undertone Acc: 0.9404 | Shade Acc: 0.8246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2839/2839 [10:24<00:00,  4.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 | Loss: 2179.9393 | Skin Acc: 0.8911 | Undertone Acc: 0.9490 | Shade Acc: 0.8473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2839/2839 [10:27<00:00,  4.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 | Loss: 1895.7023 | Skin Acc: 0.9076 | Undertone Acc: 0.9564 | Shade Acc: 0.8691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2839/2839 [10:27<00:00,  4.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 | Loss: 1575.2746 | Skin Acc: 0.9230 | Undertone Acc: 0.9652 | Shade Acc: 0.8922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2839/2839 [10:24<00:00,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 | Loss: 1256.6077 | Skin Acc: 0.9404 | Undertone Acc: 0.9729 | Shade Acc: 0.9160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2839/2839 [10:39<00:00,  4.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 | Loss: 989.3372 | Skin Acc: 0.9536 | Undertone Acc: 0.9792 | Shade Acc: 0.9350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2839/2839 [10:25<00:00,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 | Loss: 776.0266 | Skin Acc: 0.9644 | Undertone Acc: 0.9838 | Shade Acc: 0.9495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2839/2839 [10:27<00:00,  4.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 | Loss: 632.3020 | Skin Acc: 0.9716 | Undertone Acc: 0.9864 | Shade Acc: 0.9590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_skin = correct_under = correct_shade = 0\n",
        "\n",
        "    for images, skin_labels, under_labels, shade_labels in tqdm(train_loader):\n",
        "        images = images.to(device)\n",
        "        skin_labels = skin_labels.to(device)\n",
        "        under_labels = under_labels.to(device)\n",
        "        shade_labels = shade_labels.to(device)\n",
        "\n",
        "        out_skin, out_under, out_shade = model(images)\n",
        "\n",
        "        loss_skin = criterion_skin(out_skin, skin_labels)\n",
        "        loss_under = criterion_under(out_under, under_labels)\n",
        "        loss_shade = criterion_shade(out_shade, shade_labels)\n",
        "        loss = loss_skin + loss_under + loss_shade\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct_skin += (out_skin.argmax(1) == skin_labels).sum().item()\n",
        "        correct_under += (out_under.argmax(1) == under_labels).sum().item()\n",
        "        correct_shade += (out_shade.argmax(1) == shade_labels).sum().item()\n",
        "\n",
        "    total = len(train_dataset)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | \"\n",
        "          f\"Skin Acc: {correct_skin/total:.4f} | \"\n",
        "          f\"Undertone Acc: {correct_under/total:.4f} | \"\n",
        "          f\"Shade Acc: {correct_shade/total:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOzDw8i6aP9L",
        "outputId": "67aee4f7-c0b5-4642-be9d-7b57a66a693b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Final Accuracy | Skin Tone: 0.8720 | Undertone: 0.9141 | Foundation Shade: 0.7970\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct_skin = correct_under = correct_shade = total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y_skin, y_under, y_shade in val_loader:\n",
        "        x, y_skin, y_under, y_shade = x.to(device), y_skin.to(device), y_under.to(device), y_shade.to(device)\n",
        "        out_skin, out_under, out_shade = model(x)\n",
        "\n",
        "        pred_skin = torch.argmax(out_skin, dim=1)\n",
        "        pred_under = torch.argmax(out_under, dim=1)\n",
        "        pred_shade = torch.argmax(out_shade, dim=1)\n",
        "\n",
        "        correct_skin += (pred_skin == y_skin).sum().item()\n",
        "        correct_under += (pred_under == y_under).sum().item()\n",
        "        correct_shade += (pred_shade == y_shade).sum().item()\n",
        "        total += y_skin.size(0)\n",
        "\n",
        "print(f\"✅ Final Accuracy | Skin Tone: {correct_skin/total:.4f} | Undertone: {correct_under/total:.4f} | Foundation Shade: {correct_shade/total:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZkWAP9pay9m",
        "outputId": "7443a447-0131-4d54-e38d-45faa5dc588b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model and encoders saved.\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model state\n",
        "torch.save(model.state_dict(), \"shade_predictor_model.pth\")\n",
        "\n",
        "# Save the label encoders\n",
        "joblib.dump(skin_enc, \"skin_encoder.pkl\")\n",
        "joblib.dump(under_enc, \"under_encoder.pkl\")\n",
        "joblib.dump(shade_enc, \"shade_encoder.pkl\")\n",
        "\n",
        "print(\"✅ Model and encoders saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC5wi4pRbKTK",
        "outputId": "cfb19bf2-e571-42d4-8b48-a48f6774d10e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model and encoders loaded.\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "from torchvision import models\n",
        "\n",
        "# Load encoders\n",
        "skin_enc = joblib.load(\"skin_encoder.pkl\")\n",
        "under_enc = joblib.load(\"under_encoder.pkl\")\n",
        "shade_enc = joblib.load(\"shade_encoder.pkl\")\n",
        "\n",
        "# Re-initialize the model with correct output sizes\n",
        "model = ShadePredictorCNN(\n",
        "    num_skin=len(skin_enc.classes_),\n",
        "    num_under=len(under_enc.classes_),\n",
        "    num_shade=len(shade_enc.classes_)\n",
        ")\n",
        "\n",
        "# Load trained weights\n",
        "model.load_state_dict(torch.load(\"shade_predictor_model.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"✅ Model and encoders loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "g1RgZQa4bOKl"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],  # ResNet ImageNet mean\n",
        "        std=[0.229, 0.224, 0.225]    # ResNet ImageNet std\n",
        "    )\n",
        "])\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    return transform(image).unsqueeze(0).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e7_G_RF4bRAu"
      },
      "outputs": [],
      "source": [
        "def predict_image(image_path):\n",
        "    img_tensor = preprocess_image(image_path)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out_skin, out_under, out_shade = model(img_tensor)\n",
        "\n",
        "    # Get predicted class indices\n",
        "    skin_idx = torch.argmax(out_skin, dim=1).item()\n",
        "    under_idx = torch.argmax(out_under, dim=1).item()\n",
        "    shade_idx = torch.argmax(out_shade, dim=1).item()\n",
        "\n",
        "    # Decode class labels\n",
        "    skin_label = skin_enc.inverse_transform([skin_idx])[0]\n",
        "    under_label = under_enc.inverse_transform([under_idx])[0]\n",
        "    shade_label = shade_enc.inverse_transform([shade_idx])[0]\n",
        "\n",
        "    return {\n",
        "        \"Skin Tone\": skin_label,\n",
        "        \"Undertone\": under_label,\n",
        "        \"Shade\": shade_label\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import torch\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"shade_predictor_model.pth\")\n",
        "\n",
        "# Save the encoders\n",
        "joblib.dump(skin_enc, \"skin_enc.pkl\")\n",
        "joblib.dump(under_enc, \"under_enc.pkl\")\n",
        "joblib.dump(shade_enc, \"shade_enc.pkl\")\n",
        "\n",
        "print(\"✅ Model and encoders saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPSQJIdQT7Nx",
        "outputId": "198d1202-165b-48ea-963e-c9ce5cfe4781"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model and encoders saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import joblib\n",
        "\n",
        "# Load encoders\n",
        "skin_enc = joblib.load(\"skin_enc.pkl\")\n",
        "under_enc = joblib.load(\"under_enc.pkl\")\n",
        "shade_enc = joblib.load(\"shade_enc.pkl\")\n",
        "\n",
        "# Correct class sizes based on encoders\n",
        "num_skin = len(skin_enc.classes_)\n",
        "num_under = len(under_enc.classes_)\n",
        "num_shade = len(shade_enc.classes_)\n",
        "\n",
        "# Define the model (multi-head version)\n",
        "class ShadePredictorCNN(nn.Module):\n",
        "    def __init__(self, num_skin, num_under, num_shade):\n",
        "        super(ShadePredictorCNN, self).__init__()\n",
        "        resnet = models.resnet18(weights=None)\n",
        "        self.base = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "        self.head_skin = nn.Linear(512, num_skin)\n",
        "        self.head_under = nn.Linear(512, num_under)\n",
        "        self.head_shade = nn.Linear(512, num_shade)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.head_skin(x), self.head_under(x), self.head_shade(x)\n",
        "\n",
        "# Load model and weights\n",
        "model = ShadePredictorCNN(num_skin=num_skin, num_under=num_under, num_shade=num_shade)\n",
        "model.load_state_dict(torch.load(\"shade_predictor_model.pth\", map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "# Image transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# Prediction function\n",
        "def predict(image: Image.Image):\n",
        "    image = transform(image).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        _, _, shade_logits = model(image)\n",
        "        pred_idx = shade_logits.argmax(dim=1).item()\n",
        "        shade_label = shade_enc.inverse_transform([pred_idx])[0]\n",
        "    return f\"Recommended Foundation Shade: {shade_label}\"\n",
        "\n",
        "# Launch Gradio interface\n",
        "gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload Facial Image\"),\n",
        "    outputs=gr.Textbox(label=\"Prediction\"),\n",
        "    title=\"TrueTone Foundation Shade Predictor\",\n",
        "    description=\"Upload a facial image to get a recommended foundation shade.\"\n",
        ").launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "c6SnRmLUS4b5",
        "outputId": "4b205820-7630-4a54-f1a6-a93b0e3eba31"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c10f2dd8c8270c5dbc.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c10f2dd8c8270c5dbc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}